{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Process\n",
    "\n",
    "Here are the general steps to classify a large amount of webpages using pycaret:\n",
    "\n",
    "1. Collect and preprocess the data: Collect the webpages that need to be classified and preprocess them to extract the relevant information. This may involve cleaning the text data, removing stop words, and transforming the data into a format that can be used by pycaret.\n",
    "1. Load the data into a pandas DataFrame: Load the preprocessed data into a pandas DataFrame.\n",
    "1. Split the data into training and testing sets: Split the data into training and testing sets using the train_test_split() function from sklearn.model_selection.\n",
    "1. Set up the pycaret environment: Initialize the pycaret environment and load the data using the setup() function. This function automatically preprocesses the data and prepares it for modeling.\n",
    "1. Train and compare multiple models: Train multiple classification models using the compare_models() function. This function automatically trains and evaluates several models and selects the best one based on performance metrics.\n",
    "1. Tune the selected model: Use the tune_model() function to fine-tune the selected model and improve its performance.\n",
    "1. Evaluate the model: Evaluate the final model on the testing set using the evaluate_model() function.\n",
    "1. Use the model to classify new data: Once the final model is trained and evaluated, use it to classify new webpages using the predict_model() function.\n",
    "1. Save the model: Save the trained model to a file using the save_model() function so that it can be reused later.\n",
    "1. Deploy the model: Deploy the trained model in a production environment and use it to classify webpages as needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The preprocessing steps for webpages can vary depending on the specific requirements of your project, but some common steps include:\n",
    "\n",
    "1. Retrieving the raw HTML content of each webpage using a web scraping tool such as BeautifulSoup or Scrapy.\n",
    "1. Cleaning the HTML content by removing HTML tags, script and style tags, and other unwanted content using regular expressions or an HTML parsing library.\n",
    "1. Tokenizing the cleaned HTML content into words or phrases using a natural language processing library such as NLTK or spaCy.\n",
    "1. Normalizing the tokens by converting them to lowercase, removing punctuation, and removing stop words (common words that do not add meaning to the text)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Text Data\n",
    "\n",
    "To preprocess raw text from webpages for classification using pycaret, you can follow these steps:\n",
    "\n",
    "1. Clean the HTML tags from the text using a library such as beautifulsoup.\n",
    "1. Remove stop words and punctuation marks from the text using the nltk library.\n",
    "1. Tokenize the text into individual words using nltk.\n",
    "1. Apply stemming or lemmatization to reduce each word to its root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "webfile = \"data.csv\"\n",
    "\n",
    "with open(webfile, 'r') as f:\n",
    "    html = f.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # Extract the text content from the HTML\n",
    "    text = soup.get_text()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion\n",
    "\n",
    "1. Creating a document-term matrix or other feature representation of the preprocessed text data that can be used as input to a machine learning algorithm.\n",
    "1. Convert the processed text into a numerical representation using techniques such as bag of words, TF-IDF, or word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# load the data into a pandas dataframe\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# tokenize the text\n",
    "tokens = [word_tokenize(text) for text in data['text_clean']]\n",
    "\n",
    "# create a frequency distribution of the tokens\n",
    "freqdist = FreqDist([word for token in tokens for word in token])\n",
    "\n",
    "# convert the frequency distribution to a pandas dataframe\n",
    "df = pd.DataFrame(list(freqdist.items()), columns=['word', 'count'])\n",
    "\n",
    "#\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
